{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/multiphase/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test!\n",
      "finish0!\n",
      "finish1!\n",
      "finish2!\n",
      "finish3!\n",
      "finish4!\n",
      "finish5!\n",
      "finish6!\n",
      "finish7!\n",
      "finish8!\n",
      "finish9!\n",
      "finish10!\n",
      "finish11!\n",
      "finish12!\n",
      "finish13!\n",
      "finish14!\n",
      "finish15!\n",
      "finish16!\n",
      "finish17!\n",
      "finish18!\n",
      "finish19!\n",
      "finish20!\n",
      "finish21!\n",
      "finish22!\n",
      "finish23!\n",
      "finish24!\n",
      "finish25!\n",
      "finish26!\n",
      "finish27!\n",
      "finish28!\n",
      "finish29!\n",
      "finish30!\n",
      "finish31!\n",
      "finish32!\n",
      "finish33!\n",
      "finish34!\n",
      "finish35!\n",
      "finish36!\n",
      "finish37!\n",
      "finish38!\n",
      "finish39!\n",
      "finish40!\n",
      "finish41!\n",
      "finish42!\n",
      "finish43!\n",
      "finish44!\n",
      "finish45!\n",
      "finish46!\n",
      "finish47!\n",
      "finish48!\n",
      "finish49!\n",
      "finish50!\n",
      "finish51!\n",
      "数组元素总数： 419900000\n",
      "数组形状： (130000, 3230)\n",
      "数组的维度数目 2\n",
      "INFO:tensorflow:Restoring parameters from check_point_test2lables/data_model-133905\n",
      "预测和正确结果的维度： (500, 2)    (500, 2)\n",
      "Predict:  [ 6.828532 72.79067 ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 5.269951 73.20104 ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 4.922546 90.62346 ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 5.9868326 56.93834  ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 4.4690866 43.022884 ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 5.1550713 39.96634  ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 5.9348865 91.45914  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.753727 74.767006]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 4.930785 75.683205]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 4.395631 42.493996]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 5.8540154 72.584435 ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 4.941138 93.09644 ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 5.1225233 36.663635 ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 6.545824 71.90271 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [51.556507 39.726727]   Correct:  [110.     7.85]\n",
      "Predict:  [ 4.8425355 57.234715 ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 6.488735 42.189194]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 4.2564735 96.128784 ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 4.4698663 74.90515  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.2965055 69.41735  ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 5.0815587 58.905323 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 6.182578 42.173622]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [50.54894 40.43216]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 3.9401054 98.3244   ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 6.1000466 58.956924 ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 4.1269736 52.657764 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 4.6902447 94.80431  ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 7.2334266 42.02353  ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 6.905381 76.8105  ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.7511725 36.55039  ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [18.319473 75.42288 ]   Correct:  [110.     7.85]\n",
      "Predict:  [ 4.9648805 84.02561  ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 4.108816 73.26812 ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 3.9303527 54.05131  ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 3.9737797 92.33981  ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.7512245 56.87712  ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 5.1309566 83.14452  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [28.24408  58.919228]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 5.851376 43.977585]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 5.5629544 39.965572 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 4.976519 77.57387 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 6.746399 74.295364]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 5.287588 56.409588]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 9.871069 71.14122 ]   Correct:  [110.     7.85]\n",
      "Predict:  [ 4.754968 43.49334 ]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 4.1899056 90.76152  ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 8.88118 79.68971]   Correct:  [  9.8 101.2]\n",
      "Predict:  [ 4.837735 71.12795 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 5.049291 54.2     ]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 3.709515 56.61937 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 8.102299 31.393208]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 4.7005224 86.6449   ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 5.473734 88.82639 ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 8.338686 72.281425]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 3.9650207 50.918312 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [17.638954 63.882004]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 6.002842 55.807323]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 8.502446 56.90261 ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 7.347139 59.523384]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 4.5283127 54.298344 ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 5.886715 75.78945 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 3.7122717 75.14729  ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 4.821024 82.50865 ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.6153092 91.46951  ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 5.277109 57.852283]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 5.317499 71.375145]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [11.805464 69.44471 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 5.8306656 41.66363  ]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 5.380636 56.56263 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 5.538694 46.36787 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 4.2785263 40.97052  ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 5.476801 46.68432 ]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 4.3091364 36.163727 ]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 6.24499 73.15279]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 7.048584 55.721394]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 5.103403 86.821396]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 4.4204473 39.188114 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 6.609266 57.40508 ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 6.4668326 71.0735   ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 4.2053022 43.623116 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 6.1011405 90.54568  ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 5.341213 53.646107]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 9.733263 73.66666 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 4.3100834 84.92357  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 4.9919434 44.96128  ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 4.832182 67.84459 ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 6.5658956 95.593735 ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 6.3211913 92.48046  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.239979 97.77308 ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.1475806 95.21909  ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [24.539104 65.8731  ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 3.595217 52.265076]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 6.0987444 97.69583  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.0105195 35.712017 ]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 5.20228  49.339764]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.5680027 57.625607 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 4.668608 95.61185 ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 6.4572425 58.314884 ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 6.0961266 85.87064  ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 4.9236126 49.45643  ]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 5.7753024 76.49661  ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 4.3839974 77.32601  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 5.462369 92.23157 ]   Correct:  [  2.963 137.58 ]\n",
      "Predict:  [ 7.087036 56.02285 ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 4.675789 53.59504 ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 4.3363304 50.4692   ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 5.27615 40.82139]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 5.348805 88.58768 ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 8.658024 94.412254]   Correct:  [  6.11 124.75]\n",
      "Predict:  [  4.4180975 103.0408   ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 6.4709616 78.18515  ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [  4.2181754 105.88208  ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 5.1185217 53.699577 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 6.209303 74.755104]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 4.0949125 51.790504 ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 5.359424 55.638058]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 4.6924686 46.511383 ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 5.407054 51.243343]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 4.6988964 53.19386  ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 7.4034057 95.134796 ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.429472 99.21332 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [  4.1566153 102.11101  ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 5.2659526 76.55778  ]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 5.0665526 74.07482  ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 6.2109547 57.909523 ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 5.8632045 45.12003  ]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 7.924641 55.791836]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 4.2701745 96.15666  ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 4.337291 77.836876]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 6.8943677 73.773895 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 4.9813805 46.49211  ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 5.550191 57.81644 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 3.6916342 37.88609  ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 5.4799995 38.280014 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 5.476017 45.01254 ]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 7.1320925 77.83483  ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 7.622408 46.70131 ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 5.2301326 71.14711  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.4673576 92.45186  ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.6676974 55.27115  ]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 5.611043 70.77326 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 4.2639894 44.59568  ]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 4.6062155 53.957623 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 4.660501 39.414246]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.772933 38.600456]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 4.2994065 73.626465 ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 4.1865096 40.063282 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 7.378133 74.437546]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 8.230329 76.35565 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 4.842852 99.72742 ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.8230033 77.137085 ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 4.98923  79.786446]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 6.0421114 73.656265 ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [16.976074 72.19341 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 5.2336974 54.73534  ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [-1.5895529 90.93133  ]   Correct:  [  9.8 101.2]\n",
      "Predict:  [ 5.8680754 73.07128  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 4.1241965 87.22441  ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 5.772893 87.61572 ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 7.875902 56.691525]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 5.4463058 48.033264 ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 4.016511 89.4924  ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 5.2886877 73.960045 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 4.856413 99.996346]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.5979457 47.2332   ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 5.559208 41.479996]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 4.446159 46.0681  ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 5.1761174 43.663616 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.90075  53.104877]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 4.304925 76.95702 ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 5.1311216 95.23694  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.9753523 55.713657 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 5.500215 53.715416]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 7.2400994 36.551235 ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [27.331127 61.367683]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 4.9683933 39.125168 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 5.3030295 40.985207 ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 6.8294725 75.38958  ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 3.9818196 52.427925 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 5.6844506 44.90146  ]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 5.569281 39.819225]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 6.7523217 92.44055  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 6.408023 73.56406 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 4.3855734 70.83251  ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 5.683836 40.368446]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 5.901373 56.32785 ]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 5.3721848 86.30707  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 4.9661713 52.254936 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 7.416087 55.98828 ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 5.123128 54.695602]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 4.224411 76.88567 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 7.148015 54.698456]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 6.1603827 46.89064  ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 6.747122 78.85381 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 4.897301 59.06937 ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 5.9923463 56.87178  ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 5.762898 87.78816 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 6.9339542 57.274727 ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 3.9916468 92.87886  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 7.4716454 43.32826  ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 5.204593 52.50476 ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 7.2616506 45.394463 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 3.8143508 41.431515 ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 5.4603596 68.58141  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 8.960533 77.77939 ]   Correct:  [  9.8 101.2]\n",
      "Predict:  [ 6.040103 73.54278 ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 5.091737 86.65012 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 4.231347 74.29912 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 6.500909 73.19443 ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [  4.659658 100.14067 ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.2962255 42.14248  ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.032968 61.909046]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 5.727032 93.78268 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 4.392609 85.702194]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 4.5680695 88.75128  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.38134 46.17843]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 4.6643634 74.42434  ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 4.232241 74.99913 ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 5.0442033 99.6757   ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.258697 44.731747]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 5.344635 81.361465]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 6.7999043 40.111088 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 8.700842 76.60407 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 4.9877663 45.207375 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.654935 79.6004  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.1237283 55.17699  ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 4.5925627 77.571724 ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 5.639679 42.464424]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.4152794 79.04062  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 7.8457756 86.2528   ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 5.825248 57.027054]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 6.802488 73.17842 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 7.2450304 44.410324 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 4.4841533 43.43415  ]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 6.293804 79.851715]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 3.4145603 96.20381  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.3819714 98.397705 ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 7.6599183 76.08407  ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 5.068969 93.73622 ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [  6.2180567 100.62527  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [65.86262 30.0278 ]   Correct:  [110.     7.85]\n",
      "Predict:  [  5.0142264 100.950096 ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.236183 98.3698  ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 5.292702 46.67424 ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 7.798917 89.07145 ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 6.126148 57.470276]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 5.3343573 91.56653  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.031292 41.776386]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 5.5664434 58.207905 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 5.124121 46.195503]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.655364 80.576324]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 4.4658914 41.062725 ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [15.418471 80.93441 ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 5.834587 78.172455]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 5.0599966 53.483124 ]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 6.6566916 51.37139  ]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 3.9948673 81.29305  ]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 5.196059 36.94821 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 4.893648 84.48735 ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 5.7889266 85.20137  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.0029936 53.139397 ]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 3.0700796 50.923943 ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 8.761148 73.29942 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 5.0927706 90.33696  ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 3.6134007 52.49308  ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 5.446914 41.83446 ]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 5.776972 59.924274]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 6.4046946 84.765076 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 7.7335863 74.95651  ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 5.2485294 57.12327  ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 5.423076 94.568695]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 4.8903112 40.19411  ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 6.278633 78.09129 ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 4.085319 94.764244]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 4.7201033 73.09103  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 6.628071 76.98052 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [26.650627 69.4809  ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 4.079921 41.26741 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.432481 73.54475 ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 6.1745286 52.527954 ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 3.4837017 85.870026 ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 4.318385 39.90906 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.6915326 56.131996 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 4.570492 74.89122 ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 7.2896104 57.616806 ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 4.8573055 43.795788 ]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 5.2302575 45.59965  ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 7.809654 70.02548 ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.1146154 56.65583  ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 4.2168436 49.616558 ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 6.7997546 70.98578  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 4.8729453 95.01837  ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 3.7103138 81.975105 ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 5.0210414 88.2321   ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 5.732121 74.6815  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.610938 57.670406]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 5.7514358 39.84158  ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 4.3188467 80.82028  ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 4.693797 81.76503 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 4.3662505 96.45709  ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 7.1417656 42.16235  ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 5.081755 92.47738 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 5.5746384 54.810314 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 5.8042846 60.756863 ]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [20.093033 64.75695 ]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 7.9579186 78.34998  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 6.3465185 56.456753 ]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 5.889466 71.899895]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [  4.023053 105.12253 ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 4.280365 44.422607]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 3.4514825 51.14764  ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 5.86357 92.69273]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 4.4791026 72.96606  ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 4.9449816 44.630188 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 6.3595567 66.20488  ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 5.303339 96.959724]   Correct:  [  3.35 135.86]\n",
      "Predict:  [13.978645 69.78165 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 5.463878 76.09998 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 4.747627 53.118916]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 7.556013 41.998547]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 3.976918 37.229153]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [  4.496477 104.52605 ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 4.576868 75.75119 ]   Correct:  [  2.963 137.58 ]\n",
      "Predict:  [ 6.2884283 38.839386 ]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 4.282741 77.34833 ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 3.8633027 38.48307  ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 5.1471167 53.10334  ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 5.4317484 54.257484 ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 5.7001505 59.4741   ]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 4.6067743 69.943726 ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 5.6112003 36.4852   ]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 8.936217 71.82763 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 5.357221 43.52545 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 5.93772 72.24959]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.8280983 40.60929  ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 5.019161 38.733162]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 5.924012 47.91644 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 8.841407 76.952484]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 6.7510605 44.479958 ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 5.7116313 76.683235 ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 7.525461 74.71448 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 7.5094676 74.21942  ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 5.2256637 45.487217 ]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 4.885826 51.53562 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 4.7486157 92.46215  ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 4.221148 40.70671 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 6.7789955 42.59116  ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 5.118954 75.45978 ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 4.56615 98.69483]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 6.634386 53.588593]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 5.8032627 44.36968  ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 5.7474017 92.57027  ]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 4.6791596 40.968742 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 6.0649314 94.91738  ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.376142 51.402786]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 5.5111437 53.284542 ]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 4.67223  45.556965]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.820505 47.204254]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 5.226781 54.011364]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 5.925663 58.021526]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 4.6549664 90.71265  ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 6.1188426 81.769135 ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 7.7939305 58.485092 ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [  4.9771214 100.98028  ]   Correct:  [  2.963 137.58 ]\n",
      "Predict:  [ 4.260141 79.83418 ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 6.9368515 43.98944  ]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 8.037792 75.39818 ]   Correct:  [ 9.92 87.28]\n",
      "Predict:  [ 5.654576 59.56672 ]   Correct:  [ 6.92 50.7 ]\n",
      "Predict:  [ 5.4693375 83.44948  ]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 5.709406 92.48132 ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 5.828763 90.063416]   Correct:  [  2.963 137.58 ]\n",
      "Predict:  [ 4.2857194 75.04898  ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 6.277689 71.8901  ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.2452707 50.42353  ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 5.9879975 57.244446 ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 5.4648366 52.752556 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 3.5511093 55.24024  ]   Correct:  [ 1.04 46.5 ]\n",
      "Predict:  [ 5.784683 75.988884]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 5.4094057 44.200317 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 5.7188683 97.89509  ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 4.2602844 98.80915  ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 6.2459216 46.63544  ]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 5.0415697 71.82688  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 5.67877 92.06763]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.45512  41.747063]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 7.0909224 60.826942 ]   Correct:  [ 9.9 52.8]\n",
      "Predict:  [ 5.2056866 96.50261  ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.3357663 51.687542 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 5.1904984 43.266922 ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 3.8045368 79.78699  ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 5.34997  99.500374]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [22.694672 79.573555]   Correct:  [  9.8 101.2]\n",
      "Predict:  [ 5.8992405 77.505844 ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 5.3091717 76.98905  ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 4.2269526 40.991447 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 6.328306 57.928844]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 6.253932 39.831264]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [14.128042 80.77768 ]   Correct:  [110.     7.85]\n",
      "Predict:  [ 5.9405355 58.182377 ]   Correct:  [ 6.  57.9]\n",
      "Predict:  [  3.5362902 106.32856  ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [19.33955  74.581764]   Correct:  [  6.95 125.18]\n",
      "Predict:  [ 4.123937 41.83829 ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 7.045575 60.84176 ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 6.0839586 57.57338  ]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 8.859524 41.969646]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [  3.6035755 106.94552  ]   Correct:  [  1.03 140.98]\n",
      "Predict:  [ 4.6744566 43.849365 ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 4.9347243 40.921524 ]   Correct:  [ 1.03 20.8 ]\n",
      "Predict:  [ 4.474371 90.10173 ]   Correct:  [  4.06 133.1 ]\n",
      "Predict:  [ 4.6132326 73.19462  ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 5.5941415 96.88567  ]   Correct:  [  2.963 137.58 ]\n",
      "Predict:  [ 6.1862903 47.35804  ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 4.153469 77.62785 ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 5.534892 58.635895]   Correct:  [ 4.38 58.4 ]\n",
      "Predict:  [ 4.583207 43.11195 ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 3.3246963 84.7659   ]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 4.5530443 92.65385  ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 6.3074183 39.717045 ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 5.6007013 92.326035 ]   Correct:  [  3.35 135.86]\n",
      "Predict:  [ 4.8944874 55.54411  ]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 4.9016175 97.63881  ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 5.468986 93.27886 ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 5.6015906 46.475025 ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 6.258309 74.67447 ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.4579005 55.517437 ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 5.467347 51.661316]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 6.3273435 57.86058  ]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 4.1507106 74.75859  ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 6.3225875 88.99968  ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 5.0318136 44.737198 ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 4.85727 55.94536]   Correct:  [ 2.96 47.7 ]\n",
      "Predict:  [ 4.2938986 50.105286 ]   Correct:  [ 1.485 43.6  ]\n",
      "Predict:  [ 6.979128 56.891262]   Correct:  [ 6.  57.9]\n",
      "Predict:  [ 4.453627 54.03982 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 4.9771376 56.60843  ]   Correct:  [ 2.015 46.37 ]\n",
      "Predict:  [ 7.2572055 98.48536  ]   Correct:  [110.     7.85]\n",
      "Predict:  [ 5.204567 75.31914 ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 6.345579 73.62352 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 6.122178 58.203655]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 4.7304945 46.69872  ]   Correct:  [ 2.42 21.9 ]\n",
      "Predict:  [ 5.600924 82.98558 ]   Correct:  [  5.19 128.4 ]\n",
      "Predict:  [ 4.8550286 42.815002 ]   Correct:  [ 3.89 20.8 ]\n",
      "Predict:  [ 4.3542304 71.70244  ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 5.2281404 81.243744 ]   Correct:  [  9.8 101.2]\n",
      "Predict:  [ 4.8651114 41.201668 ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 6.322562 58.3164  ]   Correct:  [ 7.95 49.3 ]\n",
      "Predict:  [ 4.1392817 94.725266 ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 4.4610767 83.66195  ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 5.6711373 71.66794  ]   Correct:  [ 4.13 87.28]\n",
      "Predict:  [ 6.3676786 75.68593  ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [22.399036 69.919754]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.0749197 43.705006 ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 5.635134 74.975   ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 4.744982 98.89893 ]   Correct:  [  1.517 136.78 ]\n",
      "Predict:  [ 5.421512 79.20998 ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 6.227437 43.496532]   Correct:  [ 6.115 20.97 ]\n",
      "Predict:  [ 4.9051714 70.80563  ]   Correct:  [ 1.035 87.28 ]\n",
      "Predict:  [ 6.433618 73.093124]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.050666 85.96709 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 1.3762417 59.497986 ]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 5.75722  40.778477]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [ 5.0080743 84.66286  ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 4.3186197 46.412033 ]   Correct:  [ 1.47 21.7 ]\n",
      "Predict:  [ 5.3886094 39.45806  ]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 6.2258625 74.73989  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 5.9060817 87.39057  ]   Correct:  [  2.54 139.98]\n",
      "Predict:  [ 5.755481 43.960533]   Correct:  [ 3.535 17.9  ]\n",
      "Predict:  [ 5.6415772 81.955536 ]   Correct:  [ 5.88 87.28]\n",
      "Predict:  [ 5.313276 53.827663]   Correct:  [ 3.88 44.3 ]\n",
      "Predict:  [ 5.228753 43.91004 ]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 5.0896945 87.6203   ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 5.1581945 88.80391  ]   Correct:  [ 2.52 87.28]\n",
      "Predict:  [ 4.806923 89.71944 ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 6.153853 59.97147 ]   Correct:  [ 4.66 87.28]\n",
      "Predict:  [ 4.4230676 79.776245 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [ 6.332267 78.737   ]   Correct:  [110.     7.85]\n",
      "Predict:  [10.789091 85.68776 ]   Correct:  [110.     7.85]\n",
      "Predict:  [ 4.604111 55.917557]   Correct:  [ 2.54 48.89]\n",
      "Predict:  [ 5.713421 41.135887]   Correct:  [ 4.92 21.25]\n",
      "Predict:  [ 4.238279 77.4388  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 7.454898 74.78584 ]   Correct:  [ 8.08 87.28]\n",
      "Predict:  [  2.8718133 103.64238  ]   Correct:  [  2.05 140.28]\n",
      "Predict:  [ 4.6779423 82.96353  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 5.3401275 47.739105 ]   Correct:  [ 4.43 21.2 ]\n",
      "Predict:  [ 4.527425 44.455685]   Correct:  [ 2.97 21.6 ]\n",
      "Predict:  [ 5.2809715 55.066605 ]   Correct:  [ 3.516 49.23 ]\n",
      "Predict:  [ 6.3484755 70.34904  ]   Correct:  [ 7.12 87.28]\n",
      "Predict:  [ 5.827397 85.62645 ]   Correct:  [  4.53 131.9 ]\n",
      "Predict:  [ 7.565192 40.72071 ]   Correct:  [ 9.87 19.92]\n",
      "Predict:  [ 4.5639067 81.42119  ]   Correct:  [ 3.58 87.28]\n",
      "Predict:  [ 4.621567 75.0719  ]   Correct:  [ 1.52 87.28]\n",
      "Predict:  [ 3.702757 86.1952  ]   Correct:  [ 2.1  87.28]\n",
      "Predict:  [ 4.6933074 41.104675 ]   Correct:  [ 1.957 21.83 ]\n",
      "Predict:  [ 5.5798855 55.169487 ]   Correct:  [ 4.84 44.5 ]\n",
      "Predict:  [ 5.3610725 71.9227   ]   Correct:  [ 2.957 87.28 ]\n",
      "Predict:  [ 6.703847 93.860504]   Correct:  [  6.11 124.75]\n",
      "Predict:  [ 5.410819 75.04475 ]   Correct:  [ 5.39 87.28]\n",
      "Predict:  [ 5.904774 44.304405]   Correct:  [ 6.96 20.3 ]\n",
      "Predict:  [ 8.132025 43.666195]   Correct:  [ 7.99 20.39]\n",
      "Predict:  [19.533998 65.545   ]   Correct:  [ 8.08 87.28]\n",
      "accuracy_rate0:  [0.9590635299682617, 0.7239828705787659, 0.7875502556562424, 0.8651492148637772, 0.49525701999664307, -3.0049238204956055, 0.22839206457138062, 0.978524973616004, 0.04333919286727905, 0.8700178116559982, 0.743773490190506, -2.7972211837768555, 0.958836741745472, 0.8101267516613007, 0.4686955213546753, 0.3640083074569702, 0.8121070563793182, 0.32422298192977905, 0.48837798833847046, 0.8634108155965805, 0.8398268073797226, 0.7737895250320435, -3.095658779144287, 0.823849081993103, 0.8815096169710159, 0.37520718574523926, 0.844767302274704, 0.7328699827194214, 0.8256154656410217, 0.8310625106096268, 0.16654068231582642, 0.6144654154777527, 0.04342079162597656, 0.4526170492172241, 0.061570823192596436, 0.831101804971695, 0.26481008529663086, -2.063896417617798, 0.7910380363464355, 0.5699346363544464, 0.6099108457565308, 0.700115829706192, 0.4961353540420532, 0.08973699808120728, 0.9266437143087387, 0.8073035776615143, 0.9062428176403046, 0.6486772894859314, -0.5058515071868896, -1.5668416023254395, 0.8209016025066376, 0.14940059185028076, -1.608262300491333, 0.8405933380126953, -1.8125200271606445, -0.18303894996643066, 0.7550744712352753, 0.858832985162735, 0.7421352565288544, 0.4701646566390991, 0.907845064997673, -0.4422839879989624, 0.3696233034133911, -0.21075165271759033, 0.4991157054901123, 0.5146650075912476, 0.5389277338981628, 0.9535021744668484, -3.1736888885498047, 0.4247173070907593, -0.9105620384216309, 0.8956338688731194, 0.7810081243515015, 0.8771053329110146, 0.9814185108989477, 0.8679256737232208, -2.291696548461914, 0.9550962038338184, 0.9082630276679993, -2.043560028076172, 0.4972560405731201, 0.6233986914157867, 0.7953882366418839, 0.6053487956523895, 0.9853773815557361, -2.6687750816345215, 0.9253853633999825, 0.6045936048030853, 0.8432718366384506, 0.7321229875087738, -1.530806303024292, -1.456939458847046, 0.17947924137115479, 0.5825970768928528, -0.14970242977142334, 0.9570770375430584, 0.1619652509689331, 0.652246743440628, 0.8689931929111481, 0.8885749951004982, 0.6016217172145844, -0.08761787414550781, 0.1564735770225525, 0.8914511054754257, 0.4203416109085083, -2.169548511505127, 0.7580675631761551, 0.40334177017211914, 0.5829748511314392, 0.2605915069580078, 0.800861582159996, -0.7806035280227661, 0.5442202389240265, 0.9439961239695549, -0.7575167417526245, 0.4757041931152344, 0.4200442433357239, 0.6064294278621674, -1.1642398834228516, 0.7883133441209793, 0.9538589753210545, 0.3635372519493103, -0.08966374397277832, 0.7732318341732025, 0.7812521755695343, 0.7885941565036774, 0.8004688322544098, -0.8148810863494873, 0.9307491183280945, 0.6949967443943024, 0.7194395065307617, 0.7328331470489502, -1.584110975265503, 0.7873562574386597, 0.763878732919693, 0.9737996160984039, 0.7722804397344589, 0.2312706708908081, 0.8948211967945099, 0.539253294467926, 0.4326695203781128, 0.7937795370817184, 0.689927339553833, -2.5247583389282227, 0.3929518461227417, -2.1540162563323975, -2.06457257270813, 0.7437633872032166, 0.8296702057123184, -0.3623669147491455, 0.9903066633269191, 0.9293497204780579, 0.7034095227718353, 0.28870218992233276, 0.231859028339386, -0.1621992588043213, 0.8241679072380066, -2.0040743350982666, -1.805466651916504, 0.7955457121133804, 0.8930273056030273, -1.8995254039764404, 0.5227128863334656, -0.3689819574356079, 0.8622061759233475, 0.12821286916732788, 0.16274434328079224, -3.0253567695617676, 0.3443411588668823, -0.8321875333786011, 0.8673021197319031, 0.6357643604278564, 0.582418829202652, 0.7335460484027863, -0.7551538944244385, 0.7138496041297913, 0.9221484884619713, 0.8385251015424728, -1.8286728858947754, 0.825526773929596, 0.9107573702931404, 0.509421318769455, 0.7930721491575241, -0.885245680809021, 0.716966986656189, 0.9835621509701014, 0.3963627219200134, 0.5875508487224579, 0.9328411743044853, 0.6796062290668488, 0.8199968934059143, 0.8991214111447334, 0.7710116356611252, 0.748214840888977, 0.34550637006759644, 0.8659459799528122, 0.8896150439977646, 0.8721955269575119, 0.6501025557518005, 0.7570056170225143, -1.5047764778137207, 0.9566594064235687, 0.42382198572158813, -0.6001713275909424, 0.9143401011824608, 0.5375053882598877, 0.9810668081045151, 0.8180594444274902, 0.9130490124225616, 0.6090572774410248, -0.18852293491363525, -0.6532684564590454, 0.8965256065130234, 0.2569011449813843, 0.636397123336792, 0.7852505147457123, -1.0686602592468262, -0.7843692302703857, 0.29414838552474976, 0.8129351586103439, 0.9915835140272975, 0.976997746154666, 0.9231631234288216, -2.842491626739502, 0.4257913827896118, -0.017215847969055176, 0.8879993855953217, -0.3304457664489746, 0.6201235055923462, 0.8711113929748535, 0.3432173728942871, 0.7379428446292877, 0.9590473622083664, -0.2253861427307129, 0.9296251609921455, 0.7537660449743271, -0.11888635158538818, 0.9480096995830536, -1.3414428234100342, 0.14386361837387085, 0.5987510681152344, -0.4459642171859741, 0.7102997004985809, -0.187066912651062, 0.877853661775589, 0.9789752960205078, 0.8224377483129501, 0.706608772277832, 0.7291225492954254, -0.11740541458129883, -1.062739610671997, -1.0380213260650635, -0.21848511695861816, 0.8194644898176193, -0.5111644268035889, 0.8905514106154442, 0.41473519802093506, 0.6642521619796753, 0.3450632095336914, 0.7220913171768188, -0.48287510871887207, -0.9519996643066406, 0.883180283010006, 0.9812659956514835, -1.474423885345459, 0.7704482078552246, 0.6810567080974579, 0.8117449432611465, 0.9571270272135735, 0.22684818506240845, -0.1350693702697754, 0.9939656839706004, 0.8818305060267448, 0.75794418156147, -0.24766826629638672, 0.7229150831699371, -2.361804485321045, -1.9610881805419922, -2.282590389251709, 0.7727820724248886, 0.8580546230077744, -2.1926069259643555, 0.15293991565704346, -2.4159343242645264, 0.7363243103027344, 0.6259390711784363, 0.6554608345031738, 0.7218241989612579, 0.272089421749115, -0.8396252393722534, 0.9550217315554619, -2.7310149669647217, 0.7962047010660172, -0.3909722566604614, 0.28891903162002563, 0.7189639508724213, 0.521481841802597, 0.5394499003887177, 0.47316503524780273, 0.696641594171524, 0.8938380256295204, 0.9791435655206442, 0.7272515594959259, 0.6748208999633789, -0.8910839557647705, 0.24328511953353882, 0.6887358725070953, 0.9073347002267838, 0.037535011768341064, 0.2312542200088501, -0.34794723987579346, 0.8702176958322525, -2.3276355266571045, 0.7287965416908264, 0.642379492521286, 0.4169136881828308, 0.590862363576889, 0.47377699613571167, 0.13085556030273438, 0.7655535191297531, -0.7053862810134888, -0.1934034824371338, 0.4553263783454895, 0.2210952639579773, 0.5516601204872131, -0.6280970573425293, 0.26110923290252686, 0.16494989395141602, 0.822282999753952, 0.7131915092468262, 0.9176125153899193, 0.9008283540606499, 0.6228223741054535, 0.9901837231591344, 0.5904861688613892, 0.5801524519920349, 0.851151168346405, 0.8912708386778831, 0.8449387550354004, 0.9713658485561609, 0.9313689693808556, 0.9293895661830902, 0.8545648157596588, 0.07644635438919067, -1.13026762008667, -2.0982019901275635, 0.8484350144863129, 0.2688691020011902, -2.4331555366516113, 0.8345139920711517, 0.8204750716686249, 0.9406549260020256, 0.7971312254667282, 0.6611631214618683, -0.6680604219436646, 0.791830986738205, 0.06932657957077026, -0.39231014251708984, -0.5939358472824097, 0.8563096672296524, -1.0685343742370605, 0.8593880236148834, 0.7872657477855682, 0.32024258375167847, -0.8082669973373413, 0.8681917041540146, 0.8102613091468811, 0.8171352297067642, -0.17037200927734375, 0.5937423706054688, 0.03281712532043457, -0.8195523023605347, 0.9323658421635628, 0.508171021938324, 0.7532072514295578, -0.15151047706604004, -1.4145283699035645, 0.27322882413864136, 0.6094073057174683, -0.2515230178833008, 0.3227226138114929, 0.9785900507122278, -0.40074753761291504, 0.9294222518801689, 0.45682603120803833, 0.7162548005580902, -0.5393593311309814, 0.4824327826499939, 0.25235748291015625, -1.6758811473846436, -1.5266776084899902, -0.3157827854156494, 0.8285450339317322, 0.8606927394866943, -2.103837490081787, 0.9452822990715504, 0.977280093356967, 0.12843674421310425, 0.990089257247746, -1.4332914352416992, -0.782669186592102, 0.2958937883377075, 0.29404962062835693, 0.9860068960115314, 0.8976214677095413, -1.498617172241211, 0.06840640306472778, -2.790994644165039, 0.8979381695389748, -2.4572298526763916, 0.11200088262557983, 0.7426239550113678, -0.7325454950332642, 0.7363260388374329, -1.1178278923034668, 0.6806760728359222, -1.0013477802276611, 0.718004435300827, 0.3281487822532654, 0.34645700454711914, -0.3910330533981323, 0.7927183657884598, -0.31470680236816406, 0.9356617778539658, -1.675353765487671, 0.9112245216965675, 0.9454427547752857, -2.0103485584259033, 0.7817750573158264, -1.4230024814605713, 0.35903048515319824, -0.8915140628814697, 0.8368120193481445, 0.24660348892211914, -0.4700433015823364, 0.06597459316253662, 0.23991650342941284, 0.8227125555276871, 0.7700853049755096, 0.0452502965927124, 0.9208239018917084, 0.7519207000732422, 0.7837345004081726, 0.5334837138652802, 0.3619154095649719, 0.7952908575534821, 0.3703615069389343, 0.8276580423116684, 0.6268433034420013, 0.9170614778995514, -1.6659631729125977, 0.2912728190422058, 0.9545205272734165, -1.1278719902038574, 0.9220258519053459, 0.98161287792027, -2.7392961978912354, 0.9058472886681557, 0.9370437785983086, 0.5418274402618408, 0.7205531895160675, 0.9253059104084969, -0.9378365278244019, 0.18565338850021362, 0.8744188994169235, -0.32522904872894287, 0.3718583583831787, 0.9594518952071667, 0.6305990517139435, 0.937245324254036, 0.9442847184836864, -0.04690265655517578, 0.9388691633939743, 0.6794306635856628, 0.5474093556404114, 0.057566046714782715, 0.09808266162872314, 0.18735778331756592, 0.8387356102466583, -0.01822817325592041, 0.9226358830928802, 0.5991154313087463, -0.2275916337966919, 0.7945535778999329, 0.4756145477294922, 0.49801719188690186, 0.8916398286819458, 0.7135989964008331, 0.7664835155010223, 0.7251656949520111, -1.0405044555664062, 0.23678237199783325, -0.39821529388427734, 0.8471311330795288, 0.18698936700820923, 0.9028073996305466, 0.9961374420672655, 0.8483870923519135, 0.9822246562689543, -0.41757404804229736]\n",
      "accuracy_rate1:  [0.8339903056621552, 0.8386920690536499, 0.6808674335479736, 0.8769558519124985, 0.008199810981750488, 0.0785413384437561, 0.6731866300106049, 0.8566339015960693, 0.8671311438083649, -0.04298067092895508, 0.8316273540258408, 0.6603521406650543, 0.27465248107910156, 0.8238165825605392, -3.060729503631592, 0.8001108318567276, -0.06911206245422363, 0.686732292175293, 0.8582166880369186, 0.7953408807516098, 0.9913472346961498, -0.06834840774536133, 0.46324658393859863, 0.7237185537815094, 0.8371415436267853, 0.9229338318109512, 0.712278813123703, -0.10961484909057617, 0.8800470009446144, 0.27998173236846924, -7.608010292053223, 0.9627132564783096, 0.8394605964422226, 0.8944301083683968, 0.6582535803318024, 0.8781633079051971, 0.9526182934641838, 0.470676064491272, 0.9882603334262967, 0.0785781741142273, 0.8887931779026985, 0.5786243677139282, 0.854162335395813, -7.0625762939453125, -0.051572561264038086, 0.7068654596805573, 0.7874477803707123, 0.8149398863315582, 0.831140786409378, 0.782379150390625, 0.42403578758239746, 0.6189805567264557, 0.6494106948375702, 0.8281556665897369, 0.9049825370311737, 0.7319203019142151, 0.8680055886507034, 0.9222990199923515, 0.8726631700992584, 0.8616699576377869, 0.8683484494686127, 0.8609910011291504, 0.945332869887352, 0.6534470319747925, 0.8248571157455444, 0.8177720606327057, 0.7956543415784836, 0.013179183006286621, 0.7835993617773056, 0.941862091422081, 0.11195766925811768, -0.226243257522583, -0.02031993865966797, 0.8381391763687134, 0.9009587243199348, 0.9947456126101315, 0.11595600843429565, 0.8677499443292618, 0.8143160194158554, 0.9381315186619759, 0.6802830398082733, 0.7890269160270691, 0.8440267890691757, 0.9730014875531197, -0.11582493782043457, 0.7773211598396301, 0.7662824392318726, 0.7011407613754272, 0.7412667572498322, 0.7153951227664948, 0.5262270271778107, 0.8760198801755905, 0.7190919518470764, 0.004915177822113037, -0.25295722484588623, 0.9867398124188185, 0.6830393671989441, 0.8955514207482338, 0.9838524833321571, -0.3328503370285034, 0.8764506578445435, 0.8859533965587616, 0.6703849732875824, 0.86363385617733, 0.8764142841100693, 0.9146408587694168, -0.010905861854553223, 0.6520512104034424, 0.7568116635084152, 0.7361108958721161, 0.8957968726754189, 0.7741049975156784, 0.909210279583931, 0.8564975410699844, 0.8121443539857864, 0.8698342889547348, -0.15330469608306885, 0.8432653695344925, 0.7799573242664337, 0.7626035809516907, 0.7726894319057465, 0.729468584060669, 0.8771514520049095, 0.8487032800912857, 0.8253646194934845, 0.986066753976047, 0.9433364272117615, 0.7030023634433746, 0.8918065652251244, 0.8452554643154144, -0.2351977825164795, 0.9900075187906623, 0.17855334281921387, 0.1142849326133728, -0.12323284149169922, 0.6217832863330841, -0.3444432020187378, 0.8151593804359436, 0.7410970628261566, 0.7523442506790161, 0.8108760714530945, -0.4913787841796875, 0.9039686545729637, 0.10508430004119873, 0.21294188499450684, 0.843566283583641, 0.07388061285018921, 0.852859154343605, 0.8748356252908707, 0.7109168767929077, 0.8837887942790985, 0.9141435250639915, 0.8439077287912369, 0.8271472752094269, 0.8525086045265198, 0.898530937731266, 0.8372053503990173, 0.6187006235122681, 0.6405594646930695, 0.9262968674302101, -0.2603888511657715, 0.6347879469394684, 0.8473882377147675, 0.7128339409828186, -0.2227388620376587, 0.07962983846664429, -0.10356628894805908, -0.0992124080657959, 0.8866902142763138, 0.8817257657647133, 0.722038984298706, 0.9540009573101997, 0.7874623686075211, 0.1650986671447754, 0.703112781047821, 0.07265174388885498, 0.07128441333770752, 0.8637669682502747, 0.8725177347660065, 0.990978455170989, 0.10113370418548584, 0.700838178396225, 0.8428512513637543, 0.8115549087524414, 0.09582805633544922, 0.9728471301496029, 0.6352647244930267, 0.9385550133883953, 0.8643350452184677, 0.7653362601995468, 0.8809082806110382, 0.8904978334903717, -0.2996882200241089, 0.9034579992294312, 0.7616484463214874, 0.878268651664257, 0.6837084591388702, 0.8382408171892166, 0.9358517229557037, -0.17511332035064697, 0.7957623302936554, -0.23618054389953613, 0.10815000534057617, 0.785763218998909, 0.7685710489749908, 0.8426074534654617, 0.6748452186584473, 0.8512731343507767, 0.838616281747818, 0.737087219953537, 0.07568585872650146, 0.7093153893947601, 0.730394721031189, 0.9819224961102009, 0.653255432844162, -0.17822766304016113, 0.8527078479528427, 0.8592934310436249, 0.8579777926206589, -0.10998797416687012, 0.9321891218423843, 0.024084270000457764, 0.8776818737387657, -0.17343151569366455, 0.9120119586586952, 0.8714053779840469, 0.8887686207890511, 0.06098514795303345, 0.9055983051657677, 0.6890302002429962, 0.8416198641061783, 0.8384328782558441, -0.18770074844360352, 0.9366864711046219, 0.9148913398385048, 0.7293693423271179, 0.7029411792755127, 0.8717239797115326, 0.6853064894676208, 0.7406541109085083, -1.825197458267212, 0.7196328639984131, 0.7390668094158173, -0.13124382495880127, 0.7115469574928284, 0.992578140925616, 0.6942117810249329, -0.008480191230773926, 0.9967106727417558, -0.10938370227813721, 0.9231934621930122, 0.10770857334136963, 0.6465442478656769, 0.8956514224410057, 0.8466007113456726, 0.88724335283041, 0.9314052909612656, 0.22364360094070435, 0.9680035710334778, 0.6459543108940125, 0.8540134131908417, 0.9048614501953125, 0.8398192524909973, 0.7035589218139648, 0.8711165487766266, 0.026676416397094727, 0.9738994371145964, 0.9711855743080378, 0.8588051497936249, 0.8024472296237946, 0.6755872070789337, 0.10851240158081055, 0.8947215229272842, 0.9142501652240753, 0.8374315649271011, 0.599536806344986, 0.5569611489772797, 0.015989840030670166, 0.8426300138235092, -0.576162576675415, 0.6451541781425476, 0.08129507303237915, 0.8518716096878052, 0.8580570667982101, 0.9087725952267647, -0.4466921091079712, -0.1922910213470459, 0.5613265037536621, 0.8122467547655106, 0.8620055019855499, 0.8133109509944916, 0.6739847958087921, 0.9392198249697685, 0.9890913991257548, 0.5496945679187775, 0.9875069316476583, 0.08453935384750366, 0.9259885773062706, 0.9368129149079323, 0.7099741697311401, -0.0677955150604248, 0.7202288508415222, 0.9385327510535717, 0.9596427902579308, 0.5173106789588928, 0.5940105020999908, 0.7313089370727539, 0.8237843215465546, 0.7493764460086823, -0.028429627418518066, -0.3570340871810913, 0.7219060659408569, 0.8359997421503067, -0.1456822156906128, 0.7461196780204773, 0.7136738002300262, 0.7995147556066513, 0.8719063252210617, 0.9135014042258263, -0.10836076736450195, 0.28437089920043945, 0.7451243698596954, 0.5505973994731903, -0.1697981357574463, 0.8862090855836868, 0.2265866994857788, 0.8867224454879761, 0.8625265508890152, 0.6635034084320068, 0.8013717532157898, 0.26012396812438965, 0.8229563534259796, -0.09256982803344727, 0.8277908861637115, -0.038619041442871094, -0.1638638973236084, -0.36041581630706787, 0.8816737532615662, -0.1814594268798828, 0.8785888627171516, 0.856032058596611, 0.8503600209951401, -0.16915678977966309, 0.9458862282335758, 0.6759917438030243, 0.04294651746749878, -0.08882594108581543, 0.8645712584257126, 0.7000626623630524, 0.9130102768540382, -0.08798491954803467, 0.7420462369918823, 0.03034883737564087, 0.7196162641048431, 0.891464576125145, -0.6248544454574585, -0.08022677898406982, 0.9820087291300297, 0.8352088630199432, 0.855591207742691, 0.663201093673706, 0.9368599429726601, 0.8923277854919434, 0.73397496342659, 0.5836685299873352, -0.15740275382995605, 0.8638654798269272, 0.8251140415668488, 0.9561122730374336, 0.6948258280754089, 0.6546257734298706, 0.8598645925521851, 0.8236720860004425, 0.9757560212165117, 0.838855043053627, 0.9209949597716331, 0.8120378851890564, 0.55931755900383, -0.12501537799835205, 0.6993505656719208, 0.7058805227279663, -0.22391235828399658, 0.8229477852582932, 0.7380170524120331, -0.33223819732666016, 0.8479745537042618, 0.6879284977912903, 0.9500803872942924, -0.0030982494354248047, 0.9141497313976288, 0.7274482846260071, 0.7862999737262726, 0.8880138099193573, 0.882092721760273, 0.029257237911224365, 0.9995018492918462, 0.10055959224700928, -8.29015064239502, 0.9951230511069298, 0.7542102634906769, 0.5957961678504944, 0.08957570791244507, 0.6970870792865753, 0.9943588599562645, -0.10690999031066895, 0.7585864812135696, -0.0022541284561157227, 0.032618939876556396, 0.6769475936889648, 0.8386184573173523, 0.7042133510112762, -0.22861361503601074, 0.8894117176532745, 0.9959607319906354, 0.01327425241470337, 0.9711950141936541, 0.677393227815628, 0.13096261024475098, 0.6795674562454224, 0.8355532884597778, 0.6960280239582062, 0.7071938216686249, -0.1221473217010498, 0.8555736690759659, 0.7266642451286316, 0.8922506868839264, 0.9993191523244604, 0.8565374910831451, 0.6931439638137817, -0.061621904373168945, 0.8271413594484329, 0.8507961630821228, 0.9825779031962156, 0.8946651220321655, 0.7792014181613922, -10.545906066894531, 0.8629598766565323, 0.8435325473546982, 0.8193984627723694, -0.13236165046691895, 0.6463051736354828, -0.05841362476348877, 0.8215219974517822, 0.8028038144111633, 0.09251540899276733, 0.8171115666627884, 0.6767057180404663, 0.9585466347634792, 0.821126714348793, 0.8671623468399048, 0.5604789853096008, -0.023379802703857422, 0.8590169548988342, 0.723051130771637, 0.9075386971235275, -0.0742267370223999, 0.8112469613552094, 0.8374556005001068, 0.9849574705585837, 0.7830233871936798, 7.462501525878906e-05, 0.9700144156813622, -0.13880324363708496, 0.17323791980743408, 0.8563232421875, 0.6243075728416443, -0.4558957815170288, 0.938995610922575, 0.7849285751581192, -0.06635487079620361, 0.9961010366678238, 0.9825399797409773, 0.6802080571651459, 0.6871158480644226, 0.9140266552567482, -8.030191421508789, -8.91563892364502, 0.8562577664852142, 0.06419354677200317, 0.8872456252574921, 0.856849730014801, 0.7388250827789307, 0.9505445994436741, -0.25184452533721924, -0.058133482933044434, 0.8814421072602272, 0.8060155659914017, 0.6491770446300507, -0.04421234130859375, 0.9328733906149864, 0.860127180814743, 0.9875710271298885, 0.11705565452575684, 0.7602362483739853, 0.8240455985069275, 0.7523888051509857, 0.8598160743713379, -0.18248307704925537, -0.14154958724975586, 0.7509738653898239]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD8CAYAAACYebj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCTb91XtaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from numpy import array\n",
    "import numpy as np\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,2,3,4\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "INPUT_NODE = 27\n",
    "OUTPUT_NODE = 8\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE_BASE = 0.001\n",
    "LEARNING_RATE_DECAY = 0.995\n",
    "REGULARIZER = 0.0001\n",
    "STEPS = 50000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "TEST_INTERVAL_SECS = 3\n",
    "MODEL_SAVE_PATH=\"check_point_test2lables\"\n",
    "MODEL_NAME=\"data_model\"\n",
    "\n",
    "\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.truncated_normal(shape, stddev=0.1,mean=0))\n",
    "    #损失函数loss含正则化regularization\n",
    "    if regularizer != None: tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.zeros(shape))\n",
    "    return b\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride[1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] =1，意思是不对样本个数和channel进行卷积\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding=\"SAME\")  # padding=\"SAME\"用零填充边界\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,1,2,1]                                                  , strides=[1,1,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "def max_pool_1x3(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,1,3,1], strides=[1,1,3,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "def forward(x, regularizer):\n",
    " \n",
    "    x = tf.reshape(x, [-1, 1, 27, 1])\n",
    "    \n",
    "    ## convl layer ##\n",
    "    W_conv1 = get_weight([1,5,1,128],None) # kernel 5*5, channel is 1, out size 32 \n",
    "    b_conv1 = get_bias([128])\n",
    "    # 第一层卷积 输入为 [batch, height,widtn ,channel] = [1,1,3228,1]\n",
    "    # 卷积参数为 \n",
    "    h_conv1 = tf.nn.relu(conv2d(x,W_conv1) + b_conv1)  # output size 28*28*32\n",
    "    #h_pool1 = max_pool_2x2(h_conv1)                          # output size 14*14*32\n",
    "    \n",
    "    ## conv2 layer ##\n",
    "    W_conv2 = get_weight([1,5,128,64],None) # kernel 5*5, in size 32, out size 64\n",
    "    b_conv2 = get_bias([64])\n",
    "    #h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)  # output size 14*14*64\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1,W_conv2) + b_conv2)  # output size 14*14*64\n",
    "\n",
    "    h_pool2 = max_pool_2x2(h_conv2)                          # output size 7*7*64\n",
    "    \n",
    "    ## conv3 layer ##\n",
    "    W_conv3 = get_weight([1,5,64,32],None) # kernel 5*5, in size 32, out size 64\n",
    "    b_conv3 = get_bias([32])\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_pool2,W_conv3) + b_conv3)  # output size 14*14*64\n",
    "    h_pool3 = max_pool_1x3(h_conv3)                          # output size 7*7*64\n",
    "    \n",
    "    \n",
    "    ## funcl layer ##\n",
    "    W_fc1 = get_weight([1*269*32, 1024], regularizer)\n",
    "    b_fc1 = get_bias([1024])\n",
    "    \n",
    "    h_pool2_flat = tf.reshape(h_pool3, [-1, 1*269*32])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob = 0.5)\n",
    "    \n",
    "    ## func2 layer ##\n",
    "    W_fc2 = get_weight([1024, OUTPUT_NODE], regularizer) #这个是全连接层，最后输出几个节点就是 分为几个类\n",
    "    b_fc2 = get_bias([OUTPUT_NODE])\n",
    "    y = (tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "    tf.add_to_collection('pred_network', y)  # 用于加载模型获取要预测的网络结构\n",
    "    \n",
    "    return W_fc1,b_fc1,h_fc1,W_fc2,b_fc2,y\n",
    "\n",
    "def backward(new_data):\n",
    "    step_train = 0\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    new_data = new_data.astype(np.float32)  # change to numpy array and float32\n",
    "    \n",
    "    np.random.shuffle(new_data)\n",
    "\n",
    "\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE],name='x')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "\n",
    "    w1, b1, y1, w2, b2,y = forward(x, REGULARIZER)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    #损失函数loss含正则化regularization\n",
    "#     ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "#     cem = tf.reduce_mean(ce)\n",
    "#     loss = cem + tf.add_n(tf.get_collection('losses'))\n",
    "    #使用Elastic Net实现回归拟合\\n\",\n",
    "    loss_least_squares = tf.reduce_mean(tf.square(y_ - y))\n",
    "    h_fc1_drop = tf.nn.dropout(y1,keep_prob = 0.5) #这个可能有问题，导致最后结果小了一半？？可能去除 dropout\n",
    "    loss_losso = tf.reduce_mean(tf.abs(tf.matmul(h_fc1_drop,w2)))\n",
    "    loss_ridge =tf.reduce_mean(tf.square(tf.matmul(h_fc1_drop,w2)))\n",
    "    loss = loss_least_squares + loss_losso + loss_ridge\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "\n",
    "    \n",
    "    #指数衰减学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        len(new_data) / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    #滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(config = config) as sess:\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "        for i in range(STEPS):\n",
    "            #每次读入BATCH_SIZE组数据和标签\n",
    "\n",
    "            if(step_train > len(new_data)):\n",
    "                step_train = 0\n",
    "\n",
    "                continue\n",
    "            else:\n",
    "                xs = new_data[step_train:step_train+BATCH_SIZE,:3228]\n",
    "\n",
    "                ys = new_data[step_train:step_train+BATCH_SIZE,3228:]\n",
    "            step_train = step_train + BATCH_SIZE\n",
    "\n",
    "\n",
    "            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict={x: xs, y_: ys, keep_prob:1})\n",
    "\n",
    "            if i % 50 == 0:\n",
    "\n",
    "                print(\"%s after %d training step(s), loss on training batch is %.4f, learning rate is %f\"%(\n",
    "                    datetime.now(),step, loss_value,learning_rate.eval()))\n",
    "#                 print(\"w1 = \", (sess.run(w1)))\n",
    "\n",
    "                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step=global_step)\n",
    "    print(\"finish！\")\n",
    "\n",
    "\n",
    "def test(new_data):\n",
    "    \n",
    "    new_data = new_data.astype(np.float32)  # change to numpy array and float32\n",
    "    np.random.shuffle(new_data) #随机选取数字测试\n",
    "    new_data = new_data[0:500]\n",
    "    with tf.Graph().as_default() as g:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "        y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        w1, b1, y1, w2, b2, y = forward(x, None)\n",
    "\n",
    "        xs = new_data[:, :3228]\n",
    "\n",
    "        ys = new_data[:, 3228:]\n",
    "\n",
    "        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        ema_restore = ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(ema_restore)\n",
    "        \n",
    "#         correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#         while True:\n",
    "        for i in range(1):\n",
    "            with tf.Session(config=config) as sess:\n",
    "                ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "#                     accuracy_score = sess.run(accuracy, feed_dict={x: xs, y_: ys, keep_prob : 0.5})\n",
    "#                     print(\"After %s training step(s), test accuracy = %g\" % (global_step, accuracy_score))\n",
    "\n",
    "                    #输出预测值y\\n\",\n",
    "                    y_Predict = sess.run(y,feed_dict={x: xs})\n",
    "        \n",
    "                    print(\"预测和正确结果的维度：\",y_Predict.shape,\"  \",ys.shape)\n",
    "                    accuracy_rate0 = []\n",
    "                    accuracy_rate1 = []\n",
    "\n",
    "                    for i in range(len(y_Predict)):\n",
    "                        print(\"Predict: \",y_Predict[i],\"  Correct: \", ys[i])\n",
    "                        accuracy_rate0.append(1- (abs(y_Predict[i][0] - ys[i][0])/ys[i][0]))#这里变成了二维\n",
    "                        accuracy_rate1.append(1- (abs(y_Predict[i][1] - ys[i][1])/ys[i][1]))\n",
    "\n",
    "                        \n",
    "#                     print(type(ys),type(y_Predict))\n",
    "                    print(\"accuracy_rate0: \",accuracy_rate0)\n",
    "                    print(\"accuracy_rate1: \",accuracy_rate1)\n",
    "                    \n",
    "                    yticks = np.arange(0,1,0.1)\n",
    "                    plt.yticks(yticks)\n",
    "                    plt.plot(accuracy_rate0)\n",
    "                    plt.plot(accuracy_rate1,color = 'blue')\n",
    "                    plt.ylabel(\"accuracy_rate\")\n",
    "                    plt.show()\n",
    "                \n",
    "            \n",
    "                else:\n",
    "                    print('No checkpoint file found')\n",
    "                    return\n",
    "            #time.sleep(TEST_INTERVAL_SECS/2)\n",
    "            \n",
    "            \n",
    "def predict(new_data):\n",
    "\n",
    "\n",
    "    new_data = new_data.astype(np.float32)  # change to numpy array and float32\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    with tf.get_default_graph().as_default() as tg:\n",
    "        x = tf.placeholder(tf.float32, [None, INPUT_NODE])\n",
    "        w1, b1, y1, w2, b2, y = forward(x, None)\n",
    "\n",
    "\n",
    "        preValue = tf.argmax(y, 1)\n",
    "\n",
    "        print(\"type of prevalue = \",type(preValue))\n",
    "\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "        variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            init_op = tf.global_variables_initializer().run()\n",
    "            \n",
    "            ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                xs = new_data[:, 0:INPUT_NODE]\n",
    "                preValue = sess.run(preValue, feed_dict={x: xs,keep_prob:0.5})\n",
    "                print(type(preValue),len(preValue))\n",
    "                #return preValue\n",
    "                \n",
    "                preValue = preValue + 1\n",
    "                \n",
    "                print(preValue)\n",
    "                data1 = DataFrame(preValue)\n",
    "                data1.to_csv('predict_dataA5.csv')\n",
    "                print(\"finish!!!\")\n",
    "\n",
    "            else:\n",
    "                print(\"No checkpoint file found\")\n",
    "                return -1\n",
    "\n",
    "def main():\n",
    "    \n",
    "    train=False\n",
    "    \n",
    "    if train == True: \n",
    "        print(\"train!\")\n",
    "        data0 = pd.read_csv(\"csv/gc1.csv\",low_memory=False).values\n",
    "        print(\"finish0!\")\n",
    "        data1 = pd.read_csv(\"csv/gc2.csv\",low_memory=False).values\n",
    "        print(\"finish1!\")\n",
    "        data2 = pd.read_csv(\"csv/gc3.csv\",low_memory=False).values\n",
    "        print(\"finish2!\")\n",
    "        data3 = pd.read_csv(\"csv/gc4.csv\",low_memory=False).values\n",
    "        print(\"finish3!\")\n",
    "        data4 = pd.read_csv(\"csv/gc5.csv\",low_memory=False).values\n",
    "        print(\"finish4!\")\n",
    "        data5 = pd.read_csv(\"csv/gc6.csv\",low_memory=False).values\n",
    "        print(\"finish5!\")\n",
    "        data6 = pd.read_csv(\"csv/gc7.csv\",low_memory=False).values\n",
    "        print(\"finish6!\")\n",
    "        data7 = pd.read_csv(\"csv/gc8.csv\",low_memory=False).values\n",
    "        print(\"finish7!\")\n",
    "        data8 = pd.read_csv(\"csv/gc9.csv\",low_memory=False).values\n",
    "        print(\"finish8!\")\n",
    "        data9 = pd.read_csv(\"csv/gc10.csv\",low_memory=False).values\n",
    "        print(\"finish9!\")\n",
    "        data10 = pd.read_csv(\"csv/gc11.csv\",low_memory=False).values\n",
    "        print(\"finish10!\")\n",
    "        data11 = pd.read_csv(\"csv/gc12.csv\",low_memory=False).values\n",
    "        print(\"finish11!\")\n",
    "        data12 = pd.read_csv(\"csv/gc13.csv\",low_memory=False).values\n",
    "        print(\"finish12!\")\n",
    "        data13 = pd.read_csv(\"csv/gc14.csv\",low_memory=False).values\n",
    "        print(\"finish13!\")\n",
    "        data14 = pd.read_csv(\"csv/gc15.csv\",low_memory=False).values\n",
    "        print(\"finish14!\")\n",
    "        data15 = pd.read_csv(\"csv/gc16.csv\",low_memory=False).values\n",
    "        print(\"finish15!\")\n",
    "        data16 = pd.read_csv(\"csv/gc17.csv\",low_memory=False).values\n",
    "        print(\"finish16!\")\n",
    "        data17 = pd.read_csv(\"csv/gc18.csv\",low_memory=False).values\n",
    "        print(\"finish17!\")\n",
    "        data18 = pd.read_csv(\"csv/gc19.csv\",low_memory=False).values\n",
    "        print(\"finish18!\")\n",
    "        data19 = pd.read_csv(\"csv/gc20.csv\",low_memory=False).values\n",
    "        print(\"finish19!\")\n",
    "        data20 = pd.read_csv(\"csv/gc21.csv\",low_memory=False).values\n",
    "        print(\"finish20!\")\n",
    "        data21 = pd.read_csv(\"csv/gc22.csv\",low_memory=False).values\n",
    "        print(\"finish21!\")\n",
    "        data22 = pd.read_csv(\"csv/gc23.csv\",low_memory=False).values\n",
    "        print(\"finish22!\")\n",
    "        data23 = pd.read_csv(\"csv/gc24.csv\",low_memory=False).values\n",
    "        print(\"finish23!\")\n",
    "        data24 = pd.read_csv(\"csv/gc25.csv\",low_memory=False).values\n",
    "        print(\"finish24!\")\n",
    "        data25 = pd.read_csv(\"csv/gc26.csv\",low_memory=False).values\n",
    "        print(\"finish25!\")\n",
    "        data26 = pd.read_csv(\"csv/gc27.csv\",low_memory=False).values\n",
    "        print(\"finish26!\")\n",
    "        data27 = pd.read_csv(\"csv/gc28.csv\",low_memory=False).values\n",
    "        print(\"finish27!\")\n",
    "        data28 = pd.read_csv(\"csv/gc29.csv\",low_memory=False).values\n",
    "        print(\"finish28!\")\n",
    "        data29 = pd.read_csv(\"csv/gc30.csv\",low_memory=False).values\n",
    "        print(\"finish29!\")\n",
    "        data30 = pd.read_csv(\"csv/gc31.csv\",low_memory=False).values\n",
    "        print(\"finish30!\")\n",
    "        data31 = pd.read_csv(\"csv/gc32.csv\",low_memory=False).values\n",
    "        print(\"finish31!\")\n",
    "        data32 = pd.read_csv(\"csv/gc33.csv\",low_memory=False).values\n",
    "        print(\"finish32!\")\n",
    "        data33 = pd.read_csv(\"csv/gc34.csv\",low_memory=False).values\n",
    "        print(\"finish33!\")\n",
    "        data34 = pd.read_csv(\"csv/gc35.csv\",low_memory=False).values\n",
    "        print(\"finish34!\")\n",
    "        data35 = pd.read_csv(\"csv/gc36.csv\",low_memory=False).values\n",
    "        print(\"finish35!\")\n",
    "        data36 = pd.read_csv(\"csv/gc37.csv\",low_memory=False).values\n",
    "        print(\"finish36!\")\n",
    "        data37 = pd.read_csv(\"csv/gc38.csv\",low_memory=False).values\n",
    "        print(\"finish37!\")\n",
    "        data38 = pd.read_csv(\"csv/gc39.csv\",low_memory=False).values\n",
    "        print(\"finish38!\")\n",
    "        data39 = pd.read_csv(\"csv/gc40.csv\",low_memory=False).values\n",
    "        print(\"finish39!\")\n",
    "        data40 = pd.read_csv(\"csv/gc41.csv\",low_memory=False).values\n",
    "        print(\"finish40!\")\n",
    "        data41 = pd.read_csv(\"csv/gc42.csv\",low_memory=False).values\n",
    "        print(\"finish41!\")\n",
    "        data42 = pd.read_csv(\"csv/gc43.csv\",low_memory=False).values\n",
    "        print(\"finish42!\")\n",
    "        data43 = pd.read_csv(\"csv/gc44.csv\",low_memory=False).values\n",
    "        print(\"finish43!\")\n",
    "        data44 = pd.read_csv(\"csv/gc45.csv\",low_memory=False).values\n",
    "        print(\"finish44!\")\n",
    "        data45 = pd.read_csv(\"csv/gc46.csv\",low_memory=False).values\n",
    "        print(\"finish45!\")\n",
    "        data46 = pd.read_csv(\"csv/gc47.csv\",low_memory=False).values\n",
    "        print(\"finish46!\")\n",
    "        data47 = pd.read_csv(\"csv/gc48.csv\",low_memory=False).values\n",
    "        print(\"finish47!\")\n",
    "        data48 = pd.read_csv(\"csv/gc49.csv\",low_memory=False).values\n",
    "        print(\"finish48!\")\n",
    "        data49 = pd.read_csv(\"csv/gc50.csv\",low_memory=False).values\n",
    "        print(\"finish49!\")\n",
    "        data50 = pd.read_csv(\"csv/gc51.csv\",low_memory=False).values\n",
    "        print(\"finish50!\")\n",
    "        data51 = pd.read_csv(\"csv/gc52.csv\",low_memory=False).values\n",
    "        print(\"finish51!\")\n",
    "        \n",
    "        data0 = data0[:-int((len(data0)/4))]\n",
    "        data1 = data1[:-int((len(data1)/4))]\n",
    "        data2 = data2[:-int((len(data2)/4))]\n",
    "        data3 = data3[:-int((len(data3)/4))]\n",
    "        data4 = data4[:-int((len(data4)/4))]\n",
    "        data5 = data5[:-int((len(data5)/4))]\n",
    "        data6 = data6[:-int((len(data6)/4))]\n",
    "        data7 = data7[:-int((len(data7)/4))]\n",
    "        data8 = data8[:-int((len(data8)/4))]\n",
    "        data9 = data9[:-int((len(data9)/4))]\n",
    "        data10 = data10[:-int((len(data10)/4))]\n",
    "        data11 = data11[:-int((len(data11)/4))]\n",
    "        data12 = data12[:-int((len(data12)/4))]\n",
    "        data13 = data13[:-int((len(data13)/4))]\n",
    "        data14 = data14[:-int((len(data14)/4))]\n",
    "        data15 = data15[:-int((len(data15)/4))]\n",
    "        data16 = data16[:-int((len(data16)/4))]\n",
    "        data17 = data17[:-int((len(data17)/4))]\n",
    "        data18 = data18[:-int((len(data18)/4))]\n",
    "        data19 = data19[:-int((len(data19)/4))]\n",
    "        data20 = data20[:-int((len(data20)/4))]\n",
    "        data21 = data21[:-int((len(data21)/4))]\n",
    "        data22 = data22[:-int((len(data22)/4))]\n",
    "        data23 = data23[:-int((len(data23)/4))]\n",
    "        data24 = data24[:-int((len(data24)/4))]\n",
    "        data25 = data25[:-int((len(data25)/4))]\n",
    "        data26 = data26[:-int((len(data26)/4))]\n",
    "        data27 = data27[:-int((len(data27)/4))]\n",
    "        data28 = data28[:-int((len(data28)/4))]\n",
    "        data29 = data29[:-int((len(data29)/4))]\n",
    "        data30 = data30[:-int((len(data30)/4))]\n",
    "        data31 = data31[:-int((len(data31)/4))]\n",
    "        data32 = data32[:-int((len(data32)/4))]\n",
    "        data33 = data33[:-int((len(data33)/4))]\n",
    "        data34 = data34[:-int((len(data34)/4))]\n",
    "        data35 = data35[:-int((len(data35)/4))]\n",
    "        data36 = data36[:-int((len(data36)/4))]\n",
    "        data37 = data37[:-int((len(data37)/4))]\n",
    "        data38 = data38[:-int((len(data38)/4))]\n",
    "        data39 = data39[:-int((len(data39)/4))]\n",
    "        data40 = data40[:-int((len(data40)/4))]\n",
    "        data41 = data41[:-int((len(data41)/4))]\n",
    "        data42 = data42[:-int((len(data42)/4))]\n",
    "        data43 = data43[:-int((len(data43)/4))]\n",
    "        data44 = data44[:-int((len(data44)/4))]\n",
    "        data45 = data45[:-int((len(data45)/4))]\n",
    "        data46 = data46[:-int((len(data46)/4))]\n",
    "        data47 = data47[:-int((len(data47)/4))]\n",
    "        data48 = data48[:-int((len(data48)/4))]\n",
    "        data49 = data49[:-int((len(data49)/4))]\n",
    "        data50 = data50[:-int((len(data50)/4))]\n",
    "        data51 = data51[:-int((len(data51)/4))]\n",
    "        \n",
    "        data = np.concatenate((data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,\n",
    "        data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,\n",
    "        data30,data31,data32,data33,data34,data35,data36,data37,data38,data39,data40,data41,data42,data43,data44,data45,\n",
    "        data46,data47,data48,data49,data50,data51),axis = 0)\n",
    "        \n",
    "        print(\"数组元素总数：\",data.size)      #打印数组尺寸，即数组元素总数  \n",
    "        print(\"数组形状：\",data.shape)         #打印数组形状 \n",
    "        print(\"数组的维度数目\",data.ndim)      #打印数组的维度数目\n",
    "        \n",
    "        backward(data)\n",
    "        \n",
    "    else:\n",
    "        print(\"test!\")\n",
    "        data0 = pd.read_csv(\"csv/gc1.csv\",low_memory=False).values\n",
    "        print(\"finish0!\")\n",
    "        data1 = pd.read_csv(\"csv/gc2.csv\",low_memory=False).values\n",
    "        print(\"finish1!\")\n",
    "        data2 = pd.read_csv(\"csv/gc3.csv\",low_memory=False).values\n",
    "        print(\"finish2!\")\n",
    "        data3 = pd.read_csv(\"csv/gc4.csv\",low_memory=False).values\n",
    "        print(\"finish3!\")\n",
    "        data4 = pd.read_csv(\"csv/gc5.csv\",low_memory=False).values\n",
    "        print(\"finish4!\")\n",
    "        data5 = pd.read_csv(\"csv/gc6.csv\",low_memory=False).values\n",
    "        print(\"finish5!\")\n",
    "        data6 = pd.read_csv(\"csv/gc7.csv\",low_memory=False).values\n",
    "        print(\"finish6!\")\n",
    "        data7 = pd.read_csv(\"csv/gc8.csv\",low_memory=False).values\n",
    "        print(\"finish7!\")\n",
    "        data8 = pd.read_csv(\"csv/gc9.csv\",low_memory=False).values\n",
    "        print(\"finish8!\")\n",
    "        data9 = pd.read_csv(\"csv/gc10.csv\",low_memory=False).values\n",
    "        print(\"finish9!\")\n",
    "        data10 = pd.read_csv(\"csv/gc11.csv\",low_memory=False).values\n",
    "        print(\"finish10!\")\n",
    "        data11 = pd.read_csv(\"csv/gc12.csv\",low_memory=False).values\n",
    "        print(\"finish11!\")\n",
    "        data12 = pd.read_csv(\"csv/gc13.csv\",low_memory=False).values\n",
    "        print(\"finish12!\")\n",
    "        data13 = pd.read_csv(\"csv/gc14.csv\",low_memory=False).values\n",
    "        print(\"finish13!\")\n",
    "        data14 = pd.read_csv(\"csv/gc15.csv\",low_memory=False).values\n",
    "        print(\"finish14!\")\n",
    "        data15 = pd.read_csv(\"csv/gc16.csv\",low_memory=False).values\n",
    "        print(\"finish15!\")\n",
    "        data16 = pd.read_csv(\"csv/gc17.csv\",low_memory=False).values\n",
    "        print(\"finish16!\")\n",
    "        data17 = pd.read_csv(\"csv/gc18.csv\",low_memory=False).values\n",
    "        print(\"finish17!\")\n",
    "        data18 = pd.read_csv(\"csv/gc19.csv\",low_memory=False).values\n",
    "        print(\"finish18!\")\n",
    "        data19 = pd.read_csv(\"csv/gc20.csv\",low_memory=False).values\n",
    "        print(\"finish19!\")\n",
    "        data20 = pd.read_csv(\"csv/gc21.csv\",low_memory=False).values\n",
    "        print(\"finish20!\")\n",
    "        data21 = pd.read_csv(\"csv/gc22.csv\",low_memory=False).values\n",
    "        print(\"finish21!\")\n",
    "        data22 = pd.read_csv(\"csv/gc23.csv\",low_memory=False).values\n",
    "        print(\"finish22!\")\n",
    "        data23 = pd.read_csv(\"csv/gc24.csv\",low_memory=False).values\n",
    "        print(\"finish23!\")\n",
    "        data24 = pd.read_csv(\"csv/gc25.csv\",low_memory=False).values\n",
    "        print(\"finish24!\")\n",
    "        data25 = pd.read_csv(\"csv/gc26.csv\",low_memory=False).values\n",
    "        print(\"finish25!\")\n",
    "        data26 = pd.read_csv(\"csv/gc27.csv\",low_memory=False).values\n",
    "        print(\"finish26!\")\n",
    "        data27 = pd.read_csv(\"csv/gc28.csv\",low_memory=False).values\n",
    "        print(\"finish27!\")\n",
    "        data28 = pd.read_csv(\"csv/gc29.csv\",low_memory=False).values\n",
    "        print(\"finish28!\")\n",
    "        data29 = pd.read_csv(\"csv/gc30.csv\",low_memory=False).values\n",
    "        print(\"finish29!\")\n",
    "        data30 = pd.read_csv(\"csv/gc31.csv\",low_memory=False).values\n",
    "        print(\"finish30!\")\n",
    "        data31 = pd.read_csv(\"csv/gc32.csv\",low_memory=False).values\n",
    "        print(\"finish31!\")\n",
    "        data32 = pd.read_csv(\"csv/gc33.csv\",low_memory=False).values\n",
    "        print(\"finish32!\")\n",
    "        data33 = pd.read_csv(\"csv/gc34.csv\",low_memory=False).values\n",
    "        print(\"finish33!\")\n",
    "        data34 = pd.read_csv(\"csv/gc35.csv\",low_memory=False).values\n",
    "        print(\"finish34!\")\n",
    "        data35 = pd.read_csv(\"csv/gc36.csv\",low_memory=False).values\n",
    "        print(\"finish35!\")\n",
    "        data36 = pd.read_csv(\"csv/gc37.csv\",low_memory=False).values\n",
    "        print(\"finish36!\")\n",
    "        data37 = pd.read_csv(\"csv/gc38.csv\",low_memory=False).values\n",
    "        print(\"finish37!\")\n",
    "        data38 = pd.read_csv(\"csv/gc39.csv\",low_memory=False).values\n",
    "        print(\"finish38!\")\n",
    "        data39 = pd.read_csv(\"csv/gc40.csv\",low_memory=False).values\n",
    "        print(\"finish39!\")\n",
    "        data40 = pd.read_csv(\"csv/gc41.csv\",low_memory=False).values\n",
    "        print(\"finish40!\")\n",
    "        data41 = pd.read_csv(\"csv/gc42.csv\",low_memory=False).values\n",
    "        print(\"finish41!\")\n",
    "        data42 = pd.read_csv(\"csv/gc43.csv\",low_memory=False).values\n",
    "        print(\"finish42!\")\n",
    "        data43 = pd.read_csv(\"csv/gc44.csv\",low_memory=False).values\n",
    "        print(\"finish43!\")\n",
    "        data44 = pd.read_csv(\"csv/gc45.csv\",low_memory=False).values\n",
    "        print(\"finish44!\")\n",
    "        data45 = pd.read_csv(\"csv/gc46.csv\",low_memory=False).values\n",
    "        print(\"finish45!\")\n",
    "        data46 = pd.read_csv(\"csv/gc47.csv\",low_memory=False).values\n",
    "        print(\"finish46!\")\n",
    "        data47 = pd.read_csv(\"csv/gc48.csv\",low_memory=False).values\n",
    "        print(\"finish47!\")\n",
    "        data48 = pd.read_csv(\"csv/gc49.csv\",low_memory=False).values\n",
    "        print(\"finish48!\")\n",
    "        data49 = pd.read_csv(\"csv/gc50.csv\",low_memory=False).values\n",
    "        print(\"finish49!\")\n",
    "        data50 = pd.read_csv(\"csv/gc51.csv\",low_memory=False).values\n",
    "        print(\"finish50!\")\n",
    "        data51 = pd.read_csv(\"csv/gc52.csv\",low_memory=False).values\n",
    "        print(\"finish51!\")\n",
    "        \n",
    "        data0 = data0[-int((len(data0)/4)):]\n",
    "        data1 = data1[-int((len(data1)/4)):]\n",
    "        data2 = data2[-int((len(data2)/4)):]\n",
    "        data3 = data3[-int((len(data3)/4)):]\n",
    "        data4 = data4[-int((len(data4)/4)):]\n",
    "        data5 = data5[-int((len(data5)/4)):]\n",
    "        data6 = data6[-int((len(data6)/4)):]\n",
    "        data7 = data7[-int((len(data7)/4)):]\n",
    "        data8 = data8[-int((len(data8)/4)):]\n",
    "        data9 = data9[-int((len(data9)/4)):]\n",
    "        data10 = data10[-int((len(data10)/4)):]\n",
    "        data11 = data11[-int((len(data11)/4)):]\n",
    "        data12 = data12[-int((len(data12)/4)):]\n",
    "        data13 = data13[-int((len(data13)/4)):]\n",
    "        data14 = data14[-int((len(data14)/4)):]\n",
    "        data15 = data15[-int((len(data15)/4)):]\n",
    "        data16 = data16[-int((len(data16)/4)):]\n",
    "        data17 = data17[-int((len(data17)/4)):]\n",
    "        data18 = data18[-int((len(data18)/4)):]\n",
    "        data19 = data19[-int((len(data19)/4)):]\n",
    "        data20 = data20[-int((len(data20)/4)):]\n",
    "        data21 = data21[-int((len(data21)/4)):]\n",
    "        data22 = data22[-int((len(data22)/4)):]\n",
    "        data23 = data23[-int((len(data23)/4)):]\n",
    "        data24 = data24[-int((len(data24)/4)):]\n",
    "        data25 = data25[-int((len(data25)/4)):]\n",
    "        data26 = data26[-int((len(data26)/4)):]\n",
    "        data27 = data27[-int((len(data27)/4)):]\n",
    "        data28 = data28[-int((len(data28)/4)):]\n",
    "        data29 = data29[-int((len(data29)/4)):]\n",
    "        data30 = data30[-int((len(data30)/4)):]\n",
    "        data31 = data31[-int((len(data31)/4)):]\n",
    "        data32 = data32[-int((len(data32)/4)):]\n",
    "        data33 = data33[-int((len(data33)/4)):]\n",
    "        data34 = data34[-int((len(data34)/4)):]\n",
    "        data35 = data35[-int((len(data35)/4)):]\n",
    "        data36 = data36[-int((len(data36)/4)):]\n",
    "        data37 = data37[-int((len(data37)/4)):]\n",
    "        data38 = data38[-int((len(data38)/4)):]\n",
    "        data39 = data39[-int((len(data39)/4)):]\n",
    "        data40 = data40[-int((len(data40)/4)):]\n",
    "        data41 = data41[-int((len(data41)/4)):]\n",
    "        data42 = data42[-int((len(data42)/4)):]\n",
    "        data43 = data43[-int((len(data43)/4)):]\n",
    "        data44 = data44[-int((len(data44)/4)):]\n",
    "        data45 = data45[-int((len(data45)/4)):]\n",
    "        data46 = data46[-int((len(data46)/4)):]\n",
    "        data47 = data47[-int((len(data47)/4)):]\n",
    "        data48 = data48[-int((len(data48)/4)):]\n",
    "        data49 = data49[-int((len(data49)/4)):]\n",
    "        data50 = data50[-int((len(data50)/4)):]\n",
    "        data51 = data51[-int((len(data51)/4)):]\n",
    "        \n",
    "        \n",
    "        data = np.concatenate((data0,data1,data2,data3,data4,data5,data6,data7,data8,data9,data10,data11,data12,data13,\n",
    "        data14,data15,data16,data17,data18,data19,data20,data21,data22,data23,data24,data25,data26,data27,data28,data29,\n",
    "        data30,data31,data32,data33,data34,data35,data36,data37,data38,data39,data40,data41,data42,data43,data44,data45,\n",
    "        data46,data47,data48,data49,data50,data51),axis = 0)\n",
    "\n",
    "        print(\"数组元素总数：\",data.size)      #打印数组尺寸，即数组元素总数  \n",
    "        print(\"数组形状：\",data.shape)         #打印数组形状 \n",
    "        print(\"数组的维度数目\",data.ndim)      #打印数组的维度数目\n",
    "        \n",
    "        test(data)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
