# 文章原网址
> https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650768584&idx=2&sn=69d030424c5f47adcad70fd64c19b30a&chksm=871a40b6b06dc9a03e5051df4bfb40612bff21de68f67f5a71f843a410fa46a6a134d0e0e4cd&mpshare=1&scene=24&srcid=&sharer_sharetime=1566547616144&sharer_shareid=c6dddb77371c457f894e87ca845a013d&key=16cdfd53c3436071a688f9b5f41c9e842c4695899cb4275ce4406189b5c98eb92a4970973782acdf08a2b9f9654b72db60dfb73664543caa71ce4c0438d24ac1e34d821d5e99976123e93e858838a76c&ascene=14&uin=MTM2MTEwNTY4NA%3D%3D&devicetype=Windows+10&version=62070158&lang=zh_CN&exportkey=AfQ3IlEv9Mxr1eLH6UveCyU%3D&pass_ticket=WI1jjU5Zss9wXYz6Cqg%2BxwaXm%2FW2CXe7lnkSiaiDbZmb07Em6c6F8c24dZ%2BDmelE

论文链接：
> https://www.nature.com/articles/s41467-019-11786-6.pdf

> 为何动物生下来没多久就能学会又跑又跳，为何我们设计的神经网络无法区分猫和狗？是时候从动物身上学习设计神经网络的经验了。本论文从 **神经科学**的角度出发，分析动物快速学习的原因，为人工智能研究提供一些可以借鉴的思想。

ANN更多的是在数学与计算方面的创新，很多【拟人化】的概念都是 **高度抽象的数学表示方法**，不过正因为动物的学习和机器的学习都希望处理相似的任务，那么相互之间还是可以借鉴的。

在这片Nature中，作者表示大部分动物行为并非 **ML学习算法**，也不是 **监督学习或非监督学习**那种范式，而是将 **模块或回路编码在了基因组中**。这样不论是发展迁移学习、还是利用模型架构编码先验知识、或者初始化限定学习方向，这篇论文都有一定的借鉴意义。(和谷歌提出的自动模型架构搜索类似，首先限定固定的模块和回路编码，然后采用不同的组合方式)

具体而言，作者表示动物生来就具有高度结构化的大脑连接，使他们可以快速学习。由于这种连接方式过于复杂，无法全部完整的记录在基因组中，其必须被 **基因组瓶颈** 压缩。这种 **基因组瓶颈** 的压缩方式可以给机器学习的快速学习机制带来启发。

## 什么是 **机器** 学习

在人工神经网络研究中，**学习** 是一个技术术语，和神经科学或心理学使用的方式不同，在人工神经网络中，学习指的是一个**提取结构的过程**，从输入数据中提取**统计学规律**，并将这种结构编码进入网络中。

这些网络参数包括了所有可以记录网络的信息。例如，在全连接神经网络中，假设有N个神经元，每个都有一个参数(如一个阈值)，还有额外的 $N^2$ 个参数用于表示神经元之间的连接强度(权重)，则共有 $N + N^2$ 个自由参数。当然，当神经元数量增加时，全连接网络参数量主要由 $N^2$主导。

现在主要有三种经典的神经网络用于从结构中提取结构，并将其编码进入网络参数中(如权重和阈值)

* 在监督学习中，数据主要由样本对组成：输入数据和对应的标签。学习的目标是找到合适的网络参数来正确预测输入样本对应的标签。
* 在非监督学习中，数据没有标签。学习的目标是在没有具体的说明规律是什么的情况下从数据中发现统计学规律(SVM、K-means等，主要依据的是数据本身统计学分布的规律)
* 在强化学习中，数据驱动行为，成功的行为回得到奖励信号。

监督学习的核心是关注 **泛化性**。当参数数量增加时，网络的表述能力--即网络能够处理的输入输出映射关系的能力也随之增长。如果网络由太多的自由参数，则网络可能会**过拟合数据分布**。

在人工神经网络研究中，网路的灵活性和其需要训练的数据之间的关系被称为 **偏差-方差权衡**。越是灵活的网络就越是性能好，但是没有足够的训练数据可能是的预测结果变得非常错误，甚至比一个简单的、性能相对较低的网络中的预测结果更差。

## 什么是 动物 学习

在神经科学中，**学习** 指的是由经验导致的长期行为变化。在这种定义下，学习就包含了动物的各种行为，例如经典的自发性条件反射以及其他一系列反射活动。尽管神经科学和机器学习中术语学习的使用会有一些重合。

对于机器学习来说，为了确保泛化性，训练这样的神经网络需要大量的数据集，例如视觉问答模型大概需要 $10^7$方标注样本，所以一个孩童需要每秒问一个问题，才能获得等量的标注数据，这显然是不可能的，所以，动物和机器的学习方法有很大的不同，它们并不是以一种 **有监督的方式** 来学习该类目标。

并且，在第一个 $10^7$ 秒中，动物获得的无标数据是非常庞大的，它们从视觉到语音也是多模态的数据。如果存在的话，如何参考这中无监督学习范式，并构建新的学习模型才能更高效地学习知识。

## 为何动物学起来这么高效

### 大脑连接的制定者

ML无监督学习无法做到高效学习，它的效果一般也没有监督学习好，那么我们为什么不探索先天模型，就像预训练那样。在动物的世界中，先天机制都是编码在基因组中。

具体而言，基因组为相互连接的神经系统绘制蓝图，这种连接包括指定哪些神经元相互连接、连接的强度又是多少，这些蓝图指定的回路为先天先天行为、以及一生中发生的任何学习过程都提供了框架。

但是在复杂动物中，基因组并没有足够的能力指定所有神经元之间的连接。例如人类大约有 3×10^9 的核苷酸，它能编码超过 1GB 的信息。不过人类约有 10^11 神经元，每个神经元甚至有 10^3 个突触，这样算下来需要 3.7×10^15 bits 来指定 10^14 个连接。基因组携带的信息要比需要的信息低了 6 个数量级。

因此基因组只能指定某些构建的规则，这种规则可能是每个神经元连接最近的几个神经元，有点类似机器学习中的初始化。但更可能的情况是，基因编码的规则是 **某些特定的回路** ，然后这些回路能应用于不同部分的脑连接构建，这样既能降低编码信息的需求，同样也能提供足够的灵活性。

## 动物学习如何帮助机器学习

### 迁移学习

研究人员发现的第一个经验是：动物行为的一部分是天生的，而非从学习中习得。动物大脑不是空空如也的，而是有了很多泛用的学习算法，已经准备学习一切。一些 AI 研究者想象到，强烈的生存选择压力使得动物只学习用于生存需要的技能。动物有倾向地快速学习特定技能的想法和 AI 研究或认知科学中的「**元学习**」和「**归纳偏置**」的思想类似。

而天生机制的重要性表明，人工神经网络想要解决新的问题时，应当尽可能从已有的相关问题中构建解决方案。确实，这一观点和「迁移学习」，人工智能领域非常活跃的领域很相关。迁移学习要求将在一个任务中预训练得到的网络迁移到另一个相关的任务上，以便加速学习过程。

虽然人工神经网络中的迁移学习需要 **转移整个权重矩阵**（或者网络中很重要的一个部分），作为开始的步骤。但是，动物大脑中的信息随着一代一代的传递而变得越来越小，因为信息必须通过基因瓶颈。经过基因瓶颈传递的信息需要进行筛选，仅保留大脑连接和可塑性规则中更为通用的部分，因此也更容易泛化。

神经科学提出，也许存在一种更高性能的机制，一种 **迁移学习的泛化机制**，不仅能够在单一感知方式，如视觉任务中迁移工作，而是能够 **跨多感官方式地工作**。

### 网络结构非常重要

另一个神经科学中发现的思想是：基因组不能编码整个数据表示或行为，也不能直接编码整个优化规则。基因组只能编码连接原则和规律，这些原则和规律会被行为和表示实例化。这说明 **连接的拓扑结构和网络架构是人工系统中的优化目标**。

然而，现在的人工神经网络研究只发掘了可能网络架构的一小部分。更高性能、类似大脑皮层的架构仍待发现。












