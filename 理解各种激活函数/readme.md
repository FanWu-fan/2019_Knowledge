## 从ReLU到GELU,一文概览神经网络中的激活函数

> 本文从激活函数的背景知识开始介绍，讲解： Sigmoid, Tanh, ReLU, LReLU, PReLU, Wish,并详细介绍了这些函数优缺点。

### sigmoid

sigmoid又叫作 Logistic激活函数，它将实数值压缩进0到1的区间内，还可以在预测概率的输出层中使用。该函数将大的负数转换成0，将大的正数转换成1.数学公式为：

![](picture/2019-12-25-20-08-10.png)

![](picture/2019-12-25-20-08-48.png)

![](picture/2019-12-25-20-08-56.png)

Sigmoid函数的三个主要缺陷：

1. 梯度消失：Sigmoid函数趋近0和1的时候变化率会变得平坦，也就是说，Sigmoid的梯度趋近于0. 神经网络使用Sigmoid激活函数进行反向传播时，输出接近0或1的神经元其梯度趋近于0.这些神经元叫坐饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新的很慢，该问题叫作梯度消失。
2. 不以零为中心： Sigmoid输出不以0为中心。
3. 计算成本昂贵：exp()函数与其他非线性激活函数相比，计算成本昂贵。

### tanh
Tanh解决了Sigmoid函数中值域期望部位0的问题：
![](picture/2019-12-25-20-14-26.png)

![](picture/2019-12-25-20-14-46.png)

Tanh函数将值压缩到-1至1的区间内，和Sigmoid不同，Tanh的函数的 **输出以零为中心**，因为区间在-1到1之间。你可以将Tanh函数想象成两个Sigmoid函数放在一起。在实践中，Tanh函数的使用优先性高于Sigmoid函数，负数输入被当作负值，零输入值的映射接近零。缺点：

1. Tanh函数也会有梯度消失的问题，因此会在饱和时也会杀死梯度。

### ReLU
为了解决梯度消失问题，使用ReLU单元。
![](picture/2019-12-25-20-33-17.png)

![](picture/2019-12-25-20-33-25.png)

$$
f(x) = max(0,x)
$$

缺点：
1. 不以零为中心：和Sigmoid激活函数类似，ReLU函数的输出不以零为中心。
2. 前向传导(forward pass)过程中,如果x小于零，则神经元保持非激活状态，且在后向传导中杀死梯度。这样权重无法得到更新，网络无法学习。当x=0时，该点的梯度未定义，但是这个问题在实现中得到了解决，通过采用左侧或右侧的梯度的方式。

### Leaky ReLU
可以解决ReLu激活函数中的梯度消失问题
![](picture/2019-12-25-20-42-31.png)

$$
f(x) = max(0.1x,x)
$$

可以进行扩展，不让x乘常数项，而是让x乘超参数

### Parametric ReLU

$$
f(x) = max(\alpha x,x)
$$

### Swish
![](picture/2019-12-25-20-45-04.png)

$$
\delta(x) = \frac{x}{1+e^{-x}}
$$

